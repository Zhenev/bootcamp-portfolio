{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d70273",
   "metadata": {},
   "source": [
    "**NOTE: in this version of the notebook, the output is cleared, to make the file lighter.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f065e",
   "metadata": {},
   "source": [
    "## Content <a id='content'></a>\n",
    "\n",
    "[Introduction](#intro)\n",
    "\n",
    "[Data overview, preprocessing, and EDA](#prep)\n",
    "\n",
    "[ML](#ml)\n",
    "\n",
    "[Summary and conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa680b",
   "metadata": {},
   "source": [
    "## Introduction <a id='intro'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23778e64",
   "metadata": {},
   "source": [
    "Sweet Lift Taxi company has collected historical data on taxi orders at airports. To attract more drivers during peak hours, we were asked to predict the amount of taxi orders for the next hour. Preliminary requirement: the RMSE metric on the test set should not be more than 48."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dfad1",
   "metadata": {},
   "source": [
    "To implement the work we will:\n",
    "\n",
    "- Load the data.\n",
    "- Check that the data is free of issues — missing data, extreme values, and so on.\n",
    "- Train different models with various hyperparameters (the test sample should be 10% of the initial dataset).\n",
    "- Draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad047b47",
   "metadata": {},
   "source": [
    "[Back to Content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d3ebf",
   "metadata": {},
   "source": [
    "## Data overview, preprocessing, and EDA <a id='prep'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e9c13",
   "metadata": {},
   "source": [
    "The data is stored in the `taxi.csv` file. The number of orders is in the `num_orders` column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d623c",
   "metadata": {},
   "source": [
    "Let's load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128b202",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db568a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from IPython.display import display\n",
    "import plotly.express as px # advanced plotting\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import warnings\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.stats.diagnostic import het_white, acorr_ljungbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint as sp_randint  # for initializing random integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ea942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a37ecc",
   "metadata": {},
   "source": [
    "### File upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13f262",
   "metadata": {},
   "source": [
    "As part of the uploading we will check memory usage and date types for a small subsample, to avoid any excessive use of the memory. We will use a load() function to avoid potential problems with the file pathes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1acd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function `load` for a csv load with try-except and a number of rows limit\n",
    "def load(filename, sep = ',', nrow = None, dtype = None, parse_dates = None):\n",
    "    \"\"\"\n",
    "    i=In addition to the file name, the function takes nrows parameter\n",
    "    for a particular number of rows to load. If None, then the file is loaded fully.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_raw = pd.read_csv(filename, sep = sep, nrows = nrow, dtype = dtype, parse_dates = parse_dates)\n",
    "    except:\n",
    "        df_raw = pd.read_csv('/'+filename, sep = sep, nrows = nrow, dtype = dtype, parse_dates = parse_dates)\n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58315874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset data types and memory usage on the first 500 rows\n",
    "data_raw = load('datasets/taxi.csv', nrow = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e47227",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3130bb",
   "metadata": {},
   "source": [
    "The column names are regular, the `datetime` column takes much of the memory as `object`, let's fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = load('datasets/taxi.csv', parse_dates = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad876033",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.info(memory_usage = 'deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748eb515",
   "metadata": {},
   "source": [
    "Great, we have ~26500 data points, no missing values, and the dataset uses 414 KB of the memory. Let's take a closer look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94877b",
   "metadata": {},
   "source": [
    "Thus, we have a half year of 10-min data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9229a19f",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a14be",
   "metadata": {},
   "source": [
    "Let's make sure that there are no explicit duplicates and we have one observation for each date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6c199",
   "metadata": {},
   "source": [
    "No explicit duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9381def",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_raw[data_raw.duplicated('datetime')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5e42c",
   "metadata": {},
   "source": [
    "No duplicateds in the `datetime` column as well. Great, now, let's turn this dataframe into time series, explore it and perform some preparations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bccbe2",
   "metadata": {},
   "source": [
    "### Time series EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.set_index('datetime')\n",
    "data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average_window = 6*24  # daily\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index,\n",
    "    y=data.num_orders,\n",
    "    name='number of drives within 10-min slots',\n",
    "    line=dict(color=px.colors.qualitative.Prism[5])\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.rolling(moving_average_window).mean().index,\n",
    "    y=data.rolling(moving_average_window).mean().num_orders,\n",
    "    name='moving average number of drives within',\n",
    "    line=dict(color=px.colors.qualitative.Prism[1])\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_x=0.1, legend_y=0.95\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8859d68",
   "metadata": {},
   "source": [
    "Looks like the data exhibits some trend and, given the nature of the data, it will be reasonable to assume sone seasonality. In addition, it looks like the variablity of the data grows with the time. Let's apply the [`seasonal_decompose()` model](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) from the time series module of the [`statsmodels` library](https://www.statsmodels.org). This is a naive model using moving averages, it can be used in either in `additive` version or in `multiplicative`. We will apply the former (default), since we will assume that the variability in the taxi rides has an additive nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12528699",
   "metadata": {},
   "source": [
    "Note: `seadonal_decompose()` is a naive model; it first estimates the trend by applying a convolution filter to the data; after removing the trend from the series, the average of the de-trended series for each period is the returned seasonal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_decomposed = seasonal_decompose(data.resample('1D').sum())  # the daily average is returned as the seasonal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae4cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposed_viz(df, titles, start_date, end_date):\n",
    "    fig = make_subplots(rows=3,cols=1)\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df.trend[start_date:end_date].index,\n",
    "        y=df.trend[start_date:end_date],\n",
    "        name=titles[1],\n",
    "        line=dict(color=px.colors.qualitative.Prism[0])),\n",
    "        row=1,\n",
    "        col=1\n",
    "        )\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df.seasonal[start_date:end_date].index,\n",
    "        y=df.seasonal[start_date:end_date],\n",
    "        name=titles[2],\n",
    "        line=dict(color=px.colors.qualitative.Prism[3])),\n",
    "        row=2,\n",
    "        col=1\n",
    "        )\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df.resid[start_date:end_date].index,\n",
    "        y=df.resid[start_date:end_date],\n",
    "        name=titles[3],\n",
    "        line=dict(color=px.colors.qualitative.Prism[4]),\n",
    "        mode=\"markers+lines\"),\n",
    "        row=3,\n",
    "        col=1\n",
    "        )\n",
    "\n",
    "    fig.update_xaxes(row=2, col=1, matches='x')\n",
    "    fig.update_xaxes(row=3, col=1, matches='x')\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend_x=0.02, legend_y=0.98,\n",
    "        title_text=titles[0]\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65217ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_titles = [\n",
    "    'Seasonality in taxi ride data',\n",
    "    'daily trend',\n",
    "    'tedrended pattern',\n",
    "    'daily residuals'\n",
    "]\n",
    "decomposed_viz(data_decomposed, daily_titles, '2018-03-01', '2018-08-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fac244",
   "metadata": {},
   "source": [
    "From this naive model, the trend part follows that the Sweet Lift Taxi company steadily grew its audience, from 1300 rides per day in the beginning of March, 2018, up to 3700 rides per day by the end of August, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691132b",
   "metadata": {},
   "source": [
    "The seasonality component demonstrates some interesting pattern:\n",
    "- The average number of rides diminishes on weekends and Tuesdays;\n",
    "- There is a peak on Mondays;\n",
    "- The numbers grow from Tuesday to Thursday.\n",
    "\n",
    "No seasonality, besides the weekly one, can be observed from this dataset, supposedly due to the limited timeframe.\n",
    "\n",
    "We lack the information on the geographical scope of the rides in the dataset; however, the seasonality component observed suggests that it can be a relatively limited area, with specific traffic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff9b5e",
   "metadata": {},
   "source": [
    "Residuals, in general, are located around the zero line, although they seem to be autocorrelated; in addition, there is a period of about one month, from March 20 to April 20, when their variablity falls down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b66b1",
   "metadata": {},
   "source": [
    "As we mentioned ealier, we should explore what seems to be growing variability in the data. Let's add rolling standard deviation and the first difference to the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daily = data.resample('1d').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d66bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daily['daily_difference'] = data_daily-data_daily.shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b43fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc7e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_sub_plots = 5\n",
    "fig = make_subplots(rows=num_of_sub_plots,cols=1, )\n",
    "for i in range(1,num_of_sub_plots):\n",
    "    fig.update_xaxes(row=i+1, col=1, matches='x')\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data_decomposed.trend.index,\n",
    "    y=data_decomposed.trend,\n",
    "    name='daily trend',\n",
    "    line=dict(color=px.colors.qualitative.Prism[0]),\n",
    "    legendgroup = '1'),\n",
    "    row=1,\n",
    "    col=1,\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data_decomposed.seasonal.index,\n",
    "    y=data_decomposed.seasonal,\n",
    "    name='daily trend',\n",
    "    line=dict(color=px.colors.qualitative.Prism[3]),\n",
    "    legendgroup = '2'),\n",
    "    row=2,\n",
    "    col=1,\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data_decomposed.resid.index,\n",
    "    y=data_decomposed.resid,\n",
    "    name='daily residuals',\n",
    "    line=dict(color=px.colors.qualitative.Prism[4]),\n",
    "    legendgroup = '3',\n",
    "    mode=\"markers+lines\"),\n",
    "    row=3,\n",
    "    col=1,\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=data.iloc[:,0].resample('1d').std().index,\n",
    "    y=data.iloc[:,0].resample('1d').std(),\n",
    "    name='',\n",
    "    marker=dict(color=px.colors.qualitative.Prism[6]),\n",
    "   legendgroup = '4'),\n",
    "    row=4,\n",
    "    col=1,\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.iloc[:,0].resample('1d').std().index,\n",
    "    y=data.iloc[:,0].resample('1d').std(),\n",
    "    name='rolling standard deviation',\n",
    "    mode=\"markers\",\n",
    "    marker=dict(color=px.colors.qualitative.Prism[6]),\n",
    "    legendgroup = '4'),\n",
    "    row=4,\n",
    "    col=1,\n",
    "    )\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data_daily.index,\n",
    "    y=data_daily.daily_difference,\n",
    "    name='daily difference in number of orders',\n",
    "    line=dict(color=px.colors.qualitative.Prism[7]),\n",
    "    legendgroup = '5',\n",
    "    mode=\"markers+lines\"),\n",
    "    row=5,\n",
    "    col=1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_x=0.95, legend_y=0.99,\n",
    "    title_text=\"Daily trend and seasonality in taxi ride data\",\n",
    "    height=850, width=980,\n",
    "    legend_bgcolor='rgba(0,0,0,0)',\n",
    "    legend_tracegroupgap = 120,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef64df1",
   "metadata": {},
   "source": [
    "The standard deviation grows with the growing number of rides; it exhibits weekly spikes as well - they start to be more expressed by the end of the period under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aca559",
   "metadata": {},
   "source": [
    "#### Data upload summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b604c8",
   "metadata": {},
   "source": [
    "1. As part of the uploading process we checked memory usage and date types for a small subsample, which allowed us to properly upload the data.\n",
    "2. The raw dataset has ~26500 10-min observation points, no missing values, no duplicates.\n",
    "3. The data exhibits a trend: the overall number of rides per day grew from 1300 in the beginning of March, 2018, up to 3700 rides per day by the end of August, 2018.\n",
    "4. The seasonality component demonstrates some interesting pattern:\n",
    "- The average number of rides diminishes on weekends and Tuesdays;\n",
    "- There is a peak on Mondays;\n",
    "- The numbers grow from Tuesday to Thursday.\n",
    "5. The standard deviation grows with the growing number of rides; it exhibits weekly spikes as well - they start to be more expressed by the end of the period under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b0f05",
   "metadata": {},
   "source": [
    "From these results, the need appears to conduct heteroscedasticity and autocorrelation tests to check our assumtions, before we dive into modeling. Let's illustrate the required testing methods on `statsmodels` naive decomposition model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b27f756",
   "metadata": {},
   "source": [
    "### Testing for Heteroscedasticity for Naive Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c649cb",
   "metadata": {},
   "source": [
    "Heteroscedasticity means that conditional variance, i.e. the variability of the dependent (target) variable `y` for each value of the explanatory variables, `t` for time series, in the data is not constant. One of heteroscedasticity forms is when the amount of fluctuation depends on the value of the target variable. It can be introduced by the measurement process or be the intrinsic characteristic of the process under investigation. Heteroscedasticity makes any model, which assumes that the conditional variance of the independent variable is constant, i.e. the residual error for each prediction is identically (and, in many cases, normally) distributed and, thus,  has the same probability distribution, unreliable: standard errors of the model's parameters can become incorrect and misleading when inference should be made with regard to the significance level of the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c2298",
   "metadata": {},
   "source": [
    "There is a family of tests for heteroscedasticity; all of them share the same approach, though employ different functions for modeling the dependence of fluctuations in the data on the target variable (you can find [intro](https://towardsdatascience.com/heteroscedasticity-is-nothing-to-be-afraid-of-730dd3f7ca1f) here). We will run the most popular, White's, test on the residuals of the decomposed initial time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a036e78",
   "metadata": {},
   "source": [
    "Let's formulate the hypothesis:\n",
    "1. Null Hypothesis: the residuals are distributed with equal variance (homoscedasticity is present)\n",
    "2. Alternative Hypothesis: the residuals are not distributed with equal variance (heteroscedasticity is present)\n",
    "3. We will use 0.05 as the signifcance level parameter (alpha); if the p-value of the test results is smaller than alpha then we can reject H0 and conclude that the data is heteroscedastic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5793b526",
   "metadata": {},
   "source": [
    "Note 1: the test output contains two alternatives:\n",
    "- original test statistic\n",
    "- statistic of the hypothesis that the error variance does not depend on the explanatory variables `X`, `t` and a constant in our case (f-statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d62d0",
   "metadata": {},
   "source": [
    "Note 2: in this we are using the residuals of the naive decomposition of the daily aggregate data from the previous section; it will be sufficient in case H1 is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform White's test\n",
    "white_test = het_white(\n",
    "    np.array(data_decomposed.resid.dropna()),                             # NaN values should be dropped\n",
    "    sm.add_constant(np.array(range(len(data_decomposed.resid.dropna())))) # simplify the time domain into an integer value\n",
    "                                                                          # and add constant\n",
    ")\n",
    "#define labels to use for output of White's test\n",
    "labels = ['Test Statistic', 'Test Statistic p-value', 'F-Statistic', 'F-Test p-value']\n",
    "#print results of White's test\n",
    "for key in dict(zip(labels, white_test)):\n",
    "    print(f'{key}: {dict(zip(labels, white_test))[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbdb74",
   "metadata": {},
   "source": [
    "Note: `data_decomposed.resid` contains NaN values at the beginning and at the end of the series, due to the calculation method, they should be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_decomposed.resid[data_decomposed.resid.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8041756",
   "metadata": {},
   "source": [
    "Both p-values of the test results are smaller than alpha: we can reject H0 and conclude that the data is heteroscedastic, which should be accounted for when modeling the process. There are two ways to tackle the heteroscedasticity of the data: use a heteroscedasticity-robust model, or transform (scale) the initial data; such transformation should turn:\n",
    "- the variance into \"more\" constant, i.e. lower the heteroscedasticity in the data;\n",
    "- the residual errors of the model into normally or \"almost\" normally distributed.\n",
    "- the relationship between the explanatory variables and the dependent variable into additive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd48d7",
   "metadata": {},
   "source": [
    "Let's proceed to autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec1c95",
   "metadata": {},
   "source": [
    "### Testing for Autocorrelation for Naive Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45173c31",
   "metadata": {},
   "source": [
    "Let’s plot the data against a time lagged version of itself for lags from 1 to 30 [days]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ce069",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lagged = data.copy()\n",
    "lag_num_week = 6*24*7 # x6 - hour, x24 - day, x7 - week\n",
    "names = ['num_orders'] + ['lag_'+str(i) for i in range(1, lag_num_week+1)]\n",
    "data_lagged = pd.concat([data_lagged['num_orders'].shift(i) for i in range(0, lag_num_week+1)],\n",
    "                        axis=1)\n",
    "data_lagged.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7dbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=data_lagged.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "        corr,\n",
    "        x=corr.columns.to_list(),\n",
    "        y=corr.columns.to_list(),\n",
    "        color_continuous_scale='viridis', zmin=-0, zmax=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fc9e4",
   "metadata": {},
   "source": [
    "We can observe the following correlations within the week time period:\n",
    "- daily correlation close to 1;\n",
    "- autocorrelation of closest lags, `t-1` to `t-7` can be characterized as high, while it diminishes down to ~0.4;\n",
    "- although in a very moderate way, correlations can be tracked within the day also: the data starts at 12 am, so the pattern shows spikes of activity around 08:00 (`t-48`) am to 10:00 am, 02:00 (`t-84`) pm t 04:00 pm, and at 07:00 (`t-114`) pm.\n",
    "- additional correlation can be observed at night, between 10:00 pm to and 03:00 (`t-132` to `t-144`) and (`t-1` to `t-18`) am."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0a887",
   "metadata": {},
   "source": [
    "Let's make a close up into \"a day in Sweet Lift Taxi's life\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lagged_short = data.copy()\n",
    "lag_num_day = 6*24 # x6 - hour, x24 - day\n",
    "names = ['num_orders'] + ['lag_'+str(i) for i in range(1, lag_num_day+1)]\n",
    "data_lagged_short = pd.concat([data_lagged_short['num_orders'].shift(i) for i in range(0, lag_num_day+1)],\n",
    "                        axis=1)\n",
    "data_lagged_short.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d58c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_short = data_lagged_short.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "        corr_short,\n",
    "        x=corr_short.columns.to_list(),\n",
    "        y=corr_short.columns.to_list(),\n",
    "        color_continuous_scale='viridis', zmin=-0, zmax=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e8251",
   "metadata": {},
   "source": [
    "Let's test for the autocorrelation. Two statistical tests - Ljung-Box and Durbin-Watson - are used roughly for to check the autocorrelation in a data series; however, Ljung-Box can be used for any lag value, while Durbin-Watson can be used just for the lag of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7adca",
   "metadata": {},
   "source": [
    "Let's formulate the hypothesis:\n",
    "1. Null Hypothesis:  the residuals are independently distributed.\n",
    "2. Alternative Hypothesis: the residuals exhibit serial correlation.\n",
    "3. We will use 0.05 as the signifcance level parameter (alpha); if the p-value of the test results is higher than alpha then we fail to reject H0 and conclude that the lagged time series are independently distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd7767",
   "metadata": {},
   "source": [
    "Note 1: in this section we are testing autocorrelation within intra-day data; thus, we should modify our decomposition to account for trend and seasonality, while applying the test to residuals of the required frequency. Let's check decomposition with the original frequency and for hourly aggregated data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a890c1",
   "metadata": {},
   "source": [
    "Note 2: To preserve the daily trend calculation, we will employ the `period` parameter of the `seasonal_decompose` model to override the default frequency of the timeseries index passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ed7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hf_decomposed = seasonal_decompose(data, period=lag_num_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55206592",
   "metadata": {},
   "source": [
    "Now, if we soom in, we can observe the trend and the \"seasonality\" on the daily level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a69225",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_day = '2018-05-06'\n",
    "end_day = '2018-05-20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    'Daily patterns in taxi ride data',\n",
    "    'daily trend',\n",
    "    'intra-day pattern',\n",
    "    'intra-day residuals'\n",
    "]\n",
    "decomposed_viz(data_hf_decomposed, titles, start_day, end_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac2959",
   "metadata": {},
   "source": [
    "Now, let's see how hourly aggregation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb445a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_decomposed = seasonal_decompose(data.resample('60min').sum(), period=lag_num_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    'Daily patterns in taxi ride data',\n",
    "    'daily trend',\n",
    "    'hourly pattern',\n",
    "    'hourly residuals'\n",
    "]\n",
    "decomposed_viz(data_hourly_decomposed, titles, start_day, end_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2731a9",
   "metadata": {},
   "source": [
    "For testing purposes, we will use the daily model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24b959",
   "metadata": {},
   "source": [
    "Note 2: the residual series has NaNs for the first and the last three weeks; thus, for testing purposes we will omit these observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84440f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_decomposed.resid[500:505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f521e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_test_hourly_df = data_hourly_decomposed.resid.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_test_hourly_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aef2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_res_hourly_df = sm.stats.acorr_ljungbox(lj_test_hourly_df,\n",
    "                                24,  # number of lags to test\n",
    "                                return_df=True) # as mentioned above, NaNs must be dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_res_hourly_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2b334",
   "metadata": {},
   "source": [
    "Let's check how many Ljung-Box p-values are higher than alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe76f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lj_res_hourly_df[lj_res_hourly_df.lb_pvalue>0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48198d7",
   "metadata": {},
   "source": [
    "For this basic decomposition model, Ljung-Box test shows that all the lags (at least within one day) are highly correlated. Let's apply the `plot_acf` function of the `statsmodels` library to visually check the things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(lj_test_hourly_df, lags = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c65d44",
   "metadata": {},
   "source": [
    "This method shows that only part of the lags have statistically significant autocorrelation, namely `t-1`, `t-2`, `t-7`, `t-8`, `t-11`, `t-12`, `t-13`, `t-22`, `t-23`, `t-24`, which we already mentioned before. We should remember though that ACF only looks at particular lags and tests randomness of each tag, while  Ljung-Box test tests whether a group of autocorrelations of a time series is random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917a28f",
   "metadata": {},
   "source": [
    "### EDA summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6b1b1",
   "metadata": {},
   "source": [
    "At the data exploration step, we showed that for further modeling we should be aware of the following characteristics of the initial time series:\n",
    "- heteroscedasticity;\n",
    "- autocorrelative nature;\n",
    "- overall nonlinear dependency on time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ab3f2",
   "metadata": {},
   "source": [
    "[Back to Content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a764f",
   "metadata": {},
   "source": [
    "## Modeling the intra-day demand for taxi rides <a id='ml'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8839e4",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282553e",
   "metadata": {},
   "source": [
    "We were asked to model hourly data; thus, we will resample the source data and, based on the EDA conclusions, add the following features:\n",
    "- 24 hourly lags,\n",
    "- day of week, week, and hour,\n",
    "- 4-hour slot rolling median (to make the model more robust)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f6d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = data.resample('60min').sum().copy()\n",
    "lag_num = 24\n",
    "names = ['num_orders'] + ['lag_'+str(i) for i in range(1, lag_num+1)]\n",
    "model_data = pd.concat([model_data['num_orders'].shift(i) for i in range(0, lag_num+1)],\n",
    "                        axis=1)\n",
    "model_data.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['dayofweek'] = model_data.index.dayofweek\n",
    "model_data['week'] = model_data.index.isocalendar().week\n",
    "model_data['hour'] = model_data.index.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1c36e",
   "metadata": {},
   "source": [
    "Rolling median should be calculated on shifted data ro avoid target leakage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bc8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['rolling_median'] = model_data['num_orders'].shift(width-1).rolling(width).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaafcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574485da",
   "metadata": {},
   "source": [
    "### Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754579d2",
   "metadata": {},
   "source": [
    "We were asked to use RMSE and achieve the metric value of not more than 48 on the test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true: pd.Series, y_pred:pd.Series) -> float:  \n",
    "    \"\"\"Calculate the RSME metric\"\"\"\n",
    "    rmse_score = mean_squared_error(y_true, y_pred) ** 0.5 \n",
    "    return rmse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21aabf",
   "metadata": {},
   "source": [
    "We will [`make_scorer()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) function to wrap `rmse` scoring function for use in `cros_val_score`. In our case, the lower is the metric value the better; thus, we should define the `greater_is_better` parameter of the `make_scorer()` function to be `False`. In this case, the scorer object will sign-flip the outcome, which should be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bdc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_score = make_scorer(rmse, greater_is_better=False)  # rmse as a user-defined scoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d06abc",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_data.drop('num_orders', axis = 1)\n",
    "Y = model_data['num_orders']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa5248",
   "metadata": {},
   "source": [
    "Time series must be split without shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584018d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, shuffle=False, random_state = 54321)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a3601c",
   "metadata": {},
   "source": [
    "### A note on cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164b0c9",
   "metadata": {},
   "source": [
    "Despite the fact that we are dealing with a time series, we still want to take advantage of the cross-validation technique is a well-established methodology for hyper-parameter tuning and feature choosing. For time series, the dependence on previous observations can cause data leakage from response variables to lag variables. Thus, in this case, we should choose a way for performing cross-validation which would be adapted to solve issues we have encountered in the EDA section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a49ee",
   "metadata": {},
   "source": [
    "In a general case of cross-validation, the training set is further split into k folds. During each iteration of the cross-validation, one fold is held as a validation set and the remaining folds are used for training. This allows us to make the best use of the data and avoid biasing the model towards patterns that may be overly represented in a given fold. Then the score obtained on all folds is averaged and the standard deviation is calculated. For the time series, the training set should be devided into two folds at each iteration, while the validation set is always ahead of the training split.\n",
    "\n",
    "One of the methods in [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which provides train/validate indices to split time series data samples that are observed at fixed time intervals, in train/validate sets. In each split, validate indices are higher than before, and thus shuffling in cross validator is inappropriate: in the kth split, it returns first k folds as train set and the (k+1)th fold as test set. However, this method of time series splitting can result in data leakage, while the model will look at future observations, which are supposed to be used as lags for validation purposes. To tackle this issue, blocked cross-validation was introduced.\n",
    "\n",
    "Blocked cross-validation works by adding margins abetween the training and validation folds in order to prevent the model from observing lag values which are used twice once as a regressor and another as a response. The blocked splitter should be [introduced explicitely](https://medium.com/sci-net/cross-validation-strategies-for-time-series-forecasting-9e6cfab91f60):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 24  # we are taking margin of 24 hours between the training fold and the validation fold\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "btscv = BlockingTimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86261218",
   "metadata": {},
   "source": [
    "[Back to Content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759e6a1",
   "metadata": {},
   "source": [
    "### Baseline Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_check = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "res = cross_val_score(baseline_check, X_train[['lag_1', 'hour']], Y_train, cv=btscv, scoring=rmse_score).mean()\n",
    "stop = time()\n",
    "print(f'The mean across 5 evaluation scores equals: {-res:.0f}.')\n",
    "modeling_time = stop - start\n",
    "print(f'Modeling time: {modeling_time}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7485a3",
   "metadata": {},
   "source": [
    "As a baseline, we run a simple Regression with only two features, `hour` abd `lag_1`, the RMSE score of this model ended up to be 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8a926",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360da80",
   "metadata": {},
   "source": [
    "To make the search for hyperparameters more feasible, we will empoloy the Random Search technique, namely the `RandomizedSearchCV` method. `RandomizedSearchCV` is relatively cost- (computationally less intensive) and time-effective (faster – less computational time), e.g. as compared to the `GridSearchCV` method: [link](https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf). To perform the cross-validation we will use the `cv` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('rf', RandomForestRegressor(random_state=12345))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83f512",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf = RandomizedSearchCV(\n",
    "    pipe,\n",
    "    param_distributions = {\"rf__max_depth\": range(2,10),\n",
    "              \"rf__min_samples_split\": sp_randint(2, 10),\n",
    "              \"rf__min_samples_leaf\": sp_randint(2, 30),\n",
    "              \"rf__max_leaf_nodes\": sp_randint(2,40),\n",
    "              \"rf__n_estimators\": sp_randint(20,60)\n",
    "             },\n",
    "    scoring=rmse_score,\n",
    "    n_iter=50,\n",
    "    cv = btscv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "rs_rf.fit(X_train, Y_train)\n",
    "stop = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbe1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time = stop - start\n",
    "print(f'Search time: {search_time}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad85f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8428a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "- rs_rf.best_score_  # remember that the scorer object sign-flips the outcome, since the lower score the better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43821a86",
   "metadata": {},
   "source": [
    "Thus, the best model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed44775",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_params = {}\n",
    "for key in rs_rf.best_params_:\n",
    "    best_rf_params[key[4:]] = rs_rf.best_params_[key]\n",
    "best_rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model_1 = RandomForestRegressor(random_state = 12345, **best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best_rf_1 = Pipeline([('model', best_rf_model_1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbbc336",
   "metadata": {},
   "source": [
    "Let's build the predicted time series to illustrate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "pipe_best_rf_1.fit(X_train, Y_train)\n",
    "stop = time()\n",
    "best_rf_fit_time = stop - start\n",
    "print(f'Best RandomForest model fit time: {best_rf_fit_time}.')\n",
    "pipe_best_rf_predictions = pipe_best_rf_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The train dataset RMSE score for the best RandomForest model: {rmse(Y_train, pipe_best_rf_predictions):.0f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151bba3c",
   "metadata": {},
   "source": [
    "The final RMSE metric value for the train dataset is 22, let's visualize the train and the predicted arrows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aec2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y.index,\n",
    "    y=Y,\n",
    "    name='actual number of drives in the dataset',\n",
    "    line=dict(color=px.colors.qualitative.Prism[3])\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_train.index,\n",
    "    y=pipe_best_rf_predictions,\n",
    "    name='predicted number of drives for the train dataset',\n",
    "    line=dict(color=px.colors.qualitative.Prism[5])\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend_x=0.1, legend_y=0.95\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec47583",
   "metadata": {},
   "source": [
    "We can see, that the model actually performs decently on the train data; however, while catching the overall trend and even seasonality, still misses on the variance spikes. Let's check out the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_train,\n",
    "    y=Y_train-pipe_best_rf_predictions,\n",
    "    name='residual vs response',\n",
    "    mode='markers',\n",
    "    line=dict(color=px.colors.qualitative.Prism[4])\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(title_text='actual number of drives in the test dataset')\n",
    "fig.update_yaxes(title_text='residuals of the Random Forest model')\n",
    "fig.update_layout(legend_x=0.1, legend_y=0.95, title = 'Residual plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc07803",
   "metadata": {},
   "source": [
    "There is obvious linear dependency of the residuals on the response variable; this is due to untreated heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c813f10",
   "metadata": {},
   "source": [
    "To tackle the problem of heteroscedasticity, we can apply one of the following methods:\n",
    "- transforming the target variable (the most common way is taking a log);\n",
    "- redefine the target variable, e.g. by taking difference as a new dependent variable;\n",
    "- using regression models which account for heteroscedasticity, like weighted regression (gives smaller weights to data points that have higher variances, which shrinks their squared residuals) or (G)ARCH (Autoregressive Conditional Heteroskedasticity) models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e143b76",
   "metadata": {},
   "source": [
    "Let's check whether we can transform our target variable by tacking log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dca40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(model_data.num_orders).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b3838",
   "metadata": {},
   "source": [
    "Unfortunately, on this path we encounter a problem: the infinite negative values. Looks like we have zero value in the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50747e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[model_data.num_orders==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6ed80",
   "metadata": {},
   "source": [
    "Right, there is one zero value. Let's check whether taking difference can tackle the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=model_data.index,\n",
    "    y=(model_data.num_orders - model_data.lag_1),\n",
    "    line=dict(color=px.colors.qualitative.Prism[8])\n",
    "    )\n",
    ")\n",
    "fig.update_yaxes(title_text='delta')\n",
    "fig.update_layout(legend_x=0.1, legend_y=0.95, title = 'Taking difference of reposnse variable')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da843473",
   "metadata": {},
   "source": [
    "This option does not work either: the growing variability keeps appearing. Let's check GARCH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939983c2",
   "metadata": {},
   "source": [
    "[Back to Content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86880e",
   "metadata": {},
   "source": [
    "### Intro to GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504feddc",
   "metadata": {},
   "source": [
    "ARCH models explicitly model the change in variance over time in a time series,  by incorporating lag squared residual errors into the model. ARCH assumes the series to be stationary, other than the change in variance, meaning it does not have a trend or seasonal component. Generalized Autoregressive Conditional Heteroskedasticity, or GARCH, is an extension of the ARCH model that incorporates a moving average component together with the autoregressive components of variance by incorporating lag variance terms and lag residual errors from a mean process.\n",
    "\n",
    "As such, the model introduces two parameters, `p` and `q`, that describe:\n",
    "- `p`: The number of lag variances to include in the GARCH model.\n",
    "- `q`: The number of lag residual errors to include in the GARCH model.\n",
    "\n",
    "A generally accepted notation for a GARCH model is to specify the GARCH() function with the p and q parameters GARCH(p, q); a GARCH model subsumes ARCH models, where a GARCH(0, q) is equivalent to an ARCH(q) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ad93d",
   "metadata": {},
   "source": [
    "What about the autoregressive components and moving average components of the mean? Based on the EDA results, we have to account for them as well. To this end, we have to apply ARIMA modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7fca7",
   "metadata": {},
   "source": [
    "### Intro to ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72df308",
   "metadata": {},
   "source": [
    "ARIMA (Integrated Autoregression and Moving Average) models use a linear combination of past observations and residuals to account for trend and seasonal components. The general form, [SARIMA (Seasonal Autoregressive Integrated Moving Average)](https://otexts.com/fpp2/seasonal-arima.html#), specifies the autoregression (AR), differencing (I), and moving average (MA) hyperparameters for the seasonal component of the series, as well as an additional parameter for the period of the seasonality; thus, configuring a [SARIMAX(p, d, q)x(P, D, Q, s)](https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMA.html) allows selecting hyperparameters for both the trend and seasonal elements of the series:\n",
    "- trend parameters\n",
    "    - `p`: Trend autoregression order,\n",
    "    - `d`: Trend difference order,\n",
    "    - `q`: Trend moving average order,\n",
    "- seasonal parameters\n",
    "    - P: Seasonal autoregressive order,\n",
    "    - D: Seasonal difference order,\n",
    "    - Q: Seasonal moving average order,\n",
    "    - s: The number of time steps for a single seasonal period, e.g. with monthly data (and `s = 12`),\n",
    "        - a seasonal first order autoregressive model would use x(t-12) to predict x(t).\n",
    "        - a seasonal second order autoregressive model would use x(t-12) and x(t-24) to predict x(t).\n",
    "    \n",
    "The SARIMA model can subsume the ARIMA, ARMA, AR, and MA models via model configuration parameters:\n",
    "- autoregressive models: AR(p)\n",
    "- moving average models: MA(q)\n",
    "- mixed autoregressive moving average models: ARMA(p, q)\n",
    "- integration models: ARIMA(p, d, q)\n",
    "- seasonal models: SARIMA(P, D, Q, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e38627",
   "metadata": {},
   "source": [
    "Additionaly, we should relate to the `t` parameter, which is responsible for the trend: can be specified as a string where `'c'` indicates a constant term, `'t'` indicates a linear trend in time, and `'ct'` includes both. Can also be specified as an iterable defining a polynomial, as in [`numpy.poly1d`](https://numpy.org/doc/stable/reference/generated/numpy.poly1d.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbdbc2",
   "metadata": {},
   "source": [
    "Our previous analysis suggested some ideas for these hyperparameter, but precise configuring of the model seems to take more time and deeper analysis. Alternatively, we can grid search the best configuration among a predefined set of possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e0820",
   "metadata": {},
   "source": [
    "### ARIMA + GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36061477",
   "metadata": {},
   "source": [
    "So, GARCH tackles the problem of correct estimate of conditional variance, i.e. deals with residuals, while ARIMA estimates the value of the target variable itseld. That means that we can combine ARIMA and GARCH to model both non-stationary and heteroscedastic time series. One way to implement this, despite the [comments](https://stats.stackexchange.com/questions/77925/procedure-for-fitting-an-arma-garch-model) of not being [consistent](https://en.wikipedia.org/wiki/Consistency_(statistics)), is to fit the GARCH model on the residuals of the ARIMA model instead of the target variable itself. The second option, less straightforward, but consistent, is to create simultaneous estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabc66d",
   "metadata": {},
   "source": [
    "Let's illustrate the ARIMA modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_rmse_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in btscv.split(Y_train):  # alternatively, tscv = TimeSeriesSplit(n_splits = ...) can be used\n",
    "    cv_train, cv_test = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "    arima =  sm.tsa.arima.ARIMA(cv_train, order=(24,0,4)).fit(method='innovations_mle')\n",
    "    \n",
    "    predictions = arima.predict(cv_test.index[0], cv_test.index[-1])\n",
    "    true_values = cv_test.values\n",
    "    arima_rmse_values.append((mean_squared_error(true_values, predictions))** 0.5 )\n",
    "    \n",
    "print(\"RMSE: {}\".format(np.mean(arima_rmse_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162284b",
   "metadata": {},
   "source": [
    "The `.summary()` method prints out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ef351",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbfa60d",
   "metadata": {},
   "source": [
    "Note: initially, `s=168` was included as well, but these models are the slowest and the final score for them is lower, so we will run with maximum seasonal parameter value fo 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_configs(seasonal=24*7):\n",
    "    models = list()\n",
    "    # define config lists\n",
    "    p_params = [2, 4]\n",
    "    d_params = [0, 1, 2]\n",
    "    q_params = [1, 2, 4]\n",
    "    P_params = [0, 1, 2]\n",
    "    D_params = [0, 1]\n",
    "    Q_params = [0, 2]\n",
    "    s_params = [24]\n",
    "    # create config instances\n",
    "    for p in p_params:\n",
    "        for d in d_params:\n",
    "            for q in q_params:\n",
    "                for P in P_params:\n",
    "                    for D in D_params:\n",
    "                        for Q in Q_params:\n",
    "                            for s in s_params:\n",
    "                                cfg = [(p,d,q), (P,D,Q,s)]\n",
    "                                models.append(cfg)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9906fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_list = sarima_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cfg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f15a0",
   "metadata": {},
   "source": [
    "We will run 216 combinations of parameters and choose the one with the lower score. Let's define a function to run the model in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_run(time_series, param_cfg):\n",
    "    rmse_values =[]\n",
    "    for train_index, test_index in btscv.split(time_series):\n",
    "        try:\n",
    "            cv_train, cv_test = time_series.iloc[train_index], time_series.iloc[test_index]\n",
    "            arima =  sm.tsa.arima.ARIMA(\n",
    "                cv_train,\n",
    "                order=param_cfg[0],\n",
    "                seasonal_order=param_cfg[1],\n",
    "                trend='c'\n",
    "            ).fit(method='innovations_mle')\n",
    "\n",
    "            predictions = arima.predict(cv_test.index[0], cv_test.index[-1])\n",
    "            true_values = cv_test.values\n",
    "            rmse_values.append((mean_squared_error(true_values, predictions))** 0.5 )\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    return np.mean(rmse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_gs_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3e275",
   "metadata": {},
   "source": [
    "We will iterate over the configuration list, but should take into account that not the all models will converge or converge fast (**NOTE: in this version of the notebook, the output was cleared, since the file ended up to be too large to be uploaded; be cautious that the cell below runs for long time; for more convenience, the `arima_best_params` value is passed manualy 5 cells below**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663487ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(cfg_list)):\n",
    "    result = None\n",
    "    print(f'Current configuration: {cfg_list[i]}')\n",
    "    try:\n",
    "        result = arima_run(Y_train, cfg_list[i])\n",
    "    except:\n",
    "        error = None\n",
    "    if result is not None:\n",
    "        print(f'Mean RMSE: {result}')\n",
    "    else:\n",
    "        print(f'Model did not converge.')\n",
    "    arima_gs_results[i] = {'configuration': cfg_list[i], 'rmse': result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec956ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arima_gs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b921f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_gs_results_list = [x[1] for x in arima_gs_results.items() if x[1]['rmse'] is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df775fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(arima_gs_results_list, key=lambda x: x['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23bbbb",
   "metadata": {},
   "source": [
    "Neither of the models showed a lower score than the Random Forest model above. Due to complexity of calculations, we will choose the second best one which consumes the least time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d48059",
   "metadata": {},
   "source": [
    "**See the NOTE above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = sorted(arima_gs_results_list, key=lambda x: x['rmse'])[1]\n",
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_best_params = [(2, 0, 1), (2, 0, 2, 24)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_best =  sm.tsa.arima.ARIMA(\n",
    "    Y_train,\n",
    "    order=arima_best_params[0],\n",
    "    seasonal_order=arima_best_params[1],\n",
    "    trend='c'\n",
    ").fit(method='innovations_mle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = arima_best.predict(Y_train.index[0], Y_train.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b013e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_residuals = Y_train.values - predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch.univariate import ZeroMean, GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model = ZeroMean(arima_residuals)  # applied when residuals from a model estimated separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e841d2",
   "metadata": {},
   "source": [
    "Note: the entire data is passed to the model in when initializing it; the model is trained up to a split data and forecasts are only produced for observations after the final observation used to estimate the model [link](https://arch.readthedocs.io/en/latest/univariate/forecasting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab968c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2018-07-13 01:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab826e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_model.volatility = GARCH(p=3, q=3\n",
    "                              )  # checked several combinations\n",
    "res = garch_model.fit(update_freq=0, disp=\"off\", last_obs=split_date)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc04908",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6134b4f6",
   "metadata": {},
   "source": [
    "For GARCH(3,3) model, all beta coefficients are significant, let's visualize the GARCH residual diagnostics [source code](https://medium.com/tej-api-financial-data-anlaysis/data-analysis-10-arima-garch-model-part-1-a011bf45f66c):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571172f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.graphics.tsaplots as sgt\n",
    "\n",
    "garch_std_resid = pd.Series(res.resid / res.conditional_volatility)\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "# Residual\n",
    "garch_std_resid.plot(ax = fig.add_subplot(3,1,1), title = 'GARCH Standardized-Residual', legend = False)\n",
    "\n",
    "# ACF/PACF\n",
    "sgt.plot_acf(garch_std_resid, zero = False, lags = 40, ax=fig.add_subplot(3,2,3))\n",
    "sgt.plot_pacf(garch_std_resid, zero = False, lags = 40, ax=fig.add_subplot(3,2,4))\n",
    "\n",
    "# QQ-Plot & Norm-Dist\n",
    "sm.qqplot(garch_std_resid, line='s', ax=fig.add_subplot(3,2,5)) \n",
    "plt.title(\"QQ Plot\")\n",
    "fig.add_subplot(3,2,6).hist(garch_std_resid, bins = 40)\n",
    "plt.title(\"Histogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f9273",
   "metadata": {},
   "source": [
    "The residual diagnostics shows that they have heavy right tail, which means they *still* do not come from a normal distribution ([more on GARCH residual diagnostics](https://data.library.virginia.edu/understanding-q-q-plots/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e2c71",
   "metadata": {},
   "source": [
    "### ARIMA-GARCH - conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c137c",
   "metadata": {},
   "source": [
    "We have run Grid Search for 200+ sets of the ARIMA model and found the best combination, in terms of time consumption and RMSE score. Neither of the ARIMA models showed higher RMSE value than the cross-validated RMSE of the initial Random Forest model, but our goal was to check whether we can improve the state by applying a GARCH model to ARIMA residuals. The study showed that the lag variance and lag residual error components can be explanatory variables of statistical significance; however, the GARCH model failed to produced normally distributed residuals as well.\n",
    "\n",
    "Despite being ambiguous and time-consuming, the results of diving into ARIMA-GARCH modeling suggest that re-working the initial Random Forest by including additional features, namely rolling standard deviation (a proxy to variance) and a number of its lags, can benefit the model. Below, we check this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be074d18",
   "metadata": {},
   "source": [
    "[Back to Content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53544b6a",
   "metadata": {},
   "source": [
    "### Re-working the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c6b56f",
   "metadata": {},
   "source": [
    "We should start from the very beginning and add the features mentioned above. We want to avoid splitting the data again, so we will use indices to construct 'augmented' `X_train` and `X_test` datasets and their target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_extra = data.resample('60min').sum().copy()\n",
    "model_data_extra = pd.concat([model_data_extra['num_orders'].shift(i) for i in range(0, lag_num+1)],\n",
    "                        axis=1)\n",
    "model_data_extra.columns = names\n",
    "model_data_extra['dayofweek'] = model_data_extra.index.dayofweek\n",
    "model_data_extra['week'] = model_data_extra.index.isocalendar().week\n",
    "model_data_extra['hour'] = model_data_extra.index.hour\n",
    "width = 4\n",
    "model_data_extra['rolling_median'] = model_data['num_orders'].shift(width-1).rolling(width).median()\n",
    "model_data_extra['rolling_std'] = model_data_extra['num_orders'].shift(width-1).rolling(width).std()\n",
    "model_data_extra['rolling_std_lag_1'] = model_data_extra['rolling_std'].shift(1)\n",
    "model_data_extra['rolling_std_lag_2'] = model_data_extra['rolling_std'].shift(2)\n",
    "model_data_extra['rolling_std_lag_3'] = model_data_extra['rolling_std'].shift(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extra = model_data_extra.drop('num_orders', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aaf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = X_train.index\n",
    "X_train_extra = X_extra.loc[X_train_indices,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indices = X_test.index\n",
    "X_test_extra = X_extra.loc[X_test_indices,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fd7bb",
   "metadata": {},
   "source": [
    "We should drop additional rows with NaN values which showed up due to shifting and adjust the target variable subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_extra.dropna(inplace=True)\n",
    "X_test_extra.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_extra = Y_train.loc[X_train_extra.index]\n",
    "Y_test_extra = Y_test.loc[X_test_extra.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa29a4cb",
   "metadata": {},
   "source": [
    "Now we have the updated `X_train` and `X_test` datasets, and can re-run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_extra = Pipeline([('rf', RandomForestRegressor(random_state=12345))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf_extra = RandomizedSearchCV(\n",
    "    pipe_extra,\n",
    "    param_distributions = {\"rf__max_depth\": range(2,10),\n",
    "              \"rf__min_samples_split\": sp_randint(2, 10),\n",
    "              \"rf__min_samples_leaf\": sp_randint(2, 30),\n",
    "              \"rf__max_leaf_nodes\": sp_randint(2,40),\n",
    "              \"rf__n_estimators\": sp_randint(20,60)\n",
    "             },\n",
    "    scoring=rmse_score,\n",
    "    n_iter=100,  # add number of iterations to make a more dense search\n",
    "    cv = btscv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fe2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "rs_rf_extra.fit(X_train_extra, Y_train_extra)\n",
    "stop = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_time_extra = stop - start\n",
    "print(f'Search time: {search_time_extra}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf_extra.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "- rs_rf_extra.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f363e1c",
   "metadata": {},
   "source": [
    "Thus, the best model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97330e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_extra_params = {}\n",
    "for key in rs_rf_extra.best_params_:\n",
    "    best_rf_extra_params[key[4:]] = rs_rf_extra.best_params_[key]\n",
    "best_rf_extra_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model_2 = RandomForestRegressor(random_state = 12345, **best_rf_extra_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df271f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best_rf_2 = Pipeline([('model', best_rf_model_2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8c340",
   "metadata": {},
   "source": [
    "Let's build the predicted time series to illustrate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a492cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "pipe_best_rf_2.fit(X_train_extra, Y_train_extra)\n",
    "stop = time()\n",
    "best_rf_fit_time = stop - start\n",
    "print(f'Best RandomForest model fit time: {best_rf_fit_time}.')\n",
    "pipe_best_rf_predictions = pipe_best_rf_2.predict(X_train_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99136f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The train datset RMSE score for the best RandomForest model: {rmse(Y_train_extra, pipe_best_rf_predictions):.0f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bf37f",
   "metadata": {},
   "source": [
    "The final RMSE metric value for the train dataset is 22, the same as in the first Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b376eb",
   "metadata": {},
   "source": [
    "OK, let's apply it to the test data subset and visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab369e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_extra_params = {}\n",
    "for key in rs_rf_extra.best_params_:\n",
    "    best_rf_extra_params[key[4:]] = rs_rf_extra.best_params_[key]\n",
    "final_rf_extra_model = RandomForestRegressor(random_state = 12345, **best_rf_extra_params)\n",
    "pipe_best_rf_extra = Pipeline([('model', final_rf_extra_model)])\n",
    "start = time()\n",
    "pipe_best_rf_extra.fit(X_train_extra, Y_train_extra)\n",
    "stop = time()\n",
    "best_rf_extra_fit_time = stop - start\n",
    "print(f'Best RandomForest model fit time: {best_rf_extra_fit_time}.')\n",
    "pipe_best_rf_extra_predictions = pipe_best_rf_extra.predict(X_test_extra)\n",
    "print(f\"The test datset RMSE score for the best RandomForest model: {rmse(Y_test_extra, pipe_best_rf_extra_predictions):.0f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc54e8",
   "metadata": {},
   "source": [
    "For the modified set of features, the final RMSE metric value for the test dataset is 44, which is better than the requested threshold of 48, altough the difference between the cross-validation score is pretty drastic, which can be indication for overfitting. Let's visualize the test and the predicted arrows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ad98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y.index,\n",
    "    y=Y,\n",
    "    name='actual number of drives in the test dataset',\n",
    "    line=dict(color=px.colors.qualitative.Prism[3])\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train_extra.index,\n",
    "    y=final_rf_extra_model.predict(X_train_extra),\n",
    "    name='predicted number of drives for the train dataset',\n",
    "    line=dict(color=px.colors.qualitative.Prism[5])\n",
    "    )\n",
    ")\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_test_extra.index,\n",
    "    y=pipe_best_rf_extra_predictions,\n",
    "    name='predicted number of drives for the test dataset',\n",
    "    line=dict(color=px.colors.qualitative.Prism[5], dash='dot')\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_x=0.1, legend_y=0.95\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce78e2a",
   "metadata": {},
   "source": [
    "We can see, that this model performs great on the train data and misses it when the variance starts growing on the test data. Let's check out the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9face5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Y_test_extra,\n",
    "    y=Y_test_extra-pipe_best_rf_extra_predictions,\n",
    "    name='residual vs response',\n",
    "    mode='markers',\n",
    "    line=dict(color=px.colors.qualitative.Prism[4])\n",
    "    )\n",
    ")\n",
    "fig.update_xaxes(title_text='actual number of drives in the test dataset')\n",
    "fig.update_yaxes(title_text='residuals of the Random Forest model')\n",
    "fig.update_layout(legend_x=0.1, legend_y=0.95, title = 'Residual plot')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ff346",
   "metadata": {},
   "source": [
    "The residuals demonstrate strong linear dependency on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a76f00",
   "metadata": {},
   "source": [
    "[Back to Content](#content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc385af",
   "metadata": {},
   "source": [
    "## Conclusions <a id='conclusions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca40e2e",
   "metadata": {},
   "source": [
    "The raw dataset of Sweet Lift Taxi has ~26500 10-min observation points, no missing values, no duplicates. Initial upload showed, that:\n",
    "1. The data exhibits a trend: the overall number of rides per day grew from 1300 in the beginning of March, 2018, up to 3700 rides per day by the end of August, 2018.\n",
    "2. There is a seasonality component:\n",
    "- The average number of rides diminishes on weekends and Tuesdays;\n",
    "- There is a peak on Mondays;\n",
    "- The numbers grow from Tuesday to Thursday.\n",
    "3. The standard deviation grows with the growing number of rides; it exhibits weekly spikes as well - they start to be more expressed by the end of the period under investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d06f26",
   "metadata": {},
   "source": [
    "At the data exploration step we conducted tests for and showed that in the further modeling we should be aware of the following characteristics of the initial time series:\n",
    "- heteroscedasticity;\n",
    "- autocorrelative nature;\n",
    "- overall nonlinear dependency on time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0dd69",
   "metadata": {},
   "source": [
    "Out initial feature engineering was focused on the following features:\n",
    "- 24 hourly lag,\n",
    "- day of week,  week, and hour,\n",
    "- 4-hour slot rolling median (to make the model more robust).\n",
    "\n",
    "For the resulting dataframe, we run a Random Forest model with hyperparameter search (GridSearchCV applied with custom cv function to account for the time series) and found a model with RMSE value of 22. We showed that the model residuals for the train dataset exhibit linear dependency on the target variable and suggested applying ARIMA-GARCH model.\n",
    "\n",
    "We performed grid search for better ARIMA parameters, modeled residuals and run a GARCH model on the residuals. Based on the results of the ARIMA-GARCH modeling, ambiguous and time-consuming though, we suggested re-working the initial Random Forest by including additional features, namely rolling standard deviation (a proxy to variance) and a number of its lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f67b3",
   "metadata": {},
   "source": [
    "While re-modeling, we added a rolling standard deviation term the list of features, including several lags of it. We re-run hyperparameter search and found a set of hyperparameters, which showed the same RMSE value of 22. On the test set, the model resulted in RMSE value of 44."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3126db",
   "metadata": {},
   "source": [
    "Further steps can include:\n",
    "- further investigation of additioanl features,\n",
    "- trying other models, besides RandomForest,\n",
    "- further investigation of ARIMA-GARCH variations potential,\n",
    "- developing a dynamic model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
