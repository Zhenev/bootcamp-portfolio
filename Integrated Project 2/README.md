# What Makes a Food Retailer App Successful

## Data:

At our disposal an event log. Each log entry is a user action or an event:

- `EventName` — event name;
- `DeviceIDHash` — unique user identifier;
- `EventTimestamp` — event time;
- `ExpId` — experiment number: 246 and 247 are the control groups, 248 is the test group.

## Goal:

Two goals were stated for the project: (1) based on the log data, to analyze the funnel to determine how users reach the purchase stage and (2) make coclusions on the results of an A/A/B test which aimed to determine a more successful font set.

## Libraries used:

pandas | 
matplotlib.pyplot |
scipy.stats |
seaborn |
math |
datetime |
cufflinks |
plotly |

NOTE: make sure the Jupyter Notebook is trusted to see the interactive plots.

## Contents

* Introduction
* Dataset
* Data preprocessing
* Data analysis
* Conclusions

## Summary

At the data pre-processing step:

* We uploaded the data (with the \t separator), found and dropped 413 duplicated entries, checked the data for column names, missing values and data types, fixed the column names, fixed the date and time column type and value representation and added the `event_date` column to use it in the further analysis.
* We checked what the time period was covered by the dataset; it turned out that the initial data covered 2 weeks, starting from Jul 25, but the first week represented 1.16% of the data only and seemed to be technical; thus, we cleaned the dataset and were left with a set of 241298 entries for one week, Aug 1, 2019 to Aug 7, 2019;
* We learned that there were five types of events in the dataset and 49% of them are main page views;
* Those 241298 events were generated by 7534 devices; we had to assume that the number of devices is a good proxy to the number of users;
* The event distribution was non-normal with 32 events on average for a device, while the median number of the events per device was equal to 19;
* Appr. 50 users (less than 0.7%) performed more than 70 purchases in the week of the data sample and they generated 11.4% of the total number of events, which deserves additional attention;
* The test group (248) had slightly higher number of devices than the control groups; however, the difference was around 2%. The distribution of the number of events by devices for each group had some minor differences.

The main findings on the funnel were:
1. It turned out that 69% of those users, who did not see an offer before starting to arrange a cart went lost; if they saw an offer afterwards, 100% of them returned and entered the offer screen, and 94% of those who saw the offer screen accomplished the payment step; only 28% of the initial sample of those who received it after they started arranging a cart or did not receive at all ended up accomplishing a payment.
2. On the other hand, for those who received an offer before they started arranging a cart, we lost only 6% at the payment stage: i.e. 94% of the initial sample of those received an offer before they started arranging a cart performed the payment.

While studying the results of the experiment, we:
1. Tested the two control group for **statistical difference is the proportions of users who performed particular actions and proper split**; the two groups should be seen as statistically identical (properly split);
2. Tested the difference between the test group (248) and the control groups, both in isolation and combined;
3. Since the results did not show any significant difference, we decided to study the aggregated cumulative daily values for two event types, main screen visits and payments, to try and clarify the picture;
4. The test group showed more visitors, but resulted in an ambiguous results on the conversion rate (for successful payments vs. visits of the main screen); thus, we took a look on the conversion rate for the combined sample of the two control groups and plotted the cummulative difference graph;
5. The cummulative difference graph demonstarted signs of no difference between the control group and the test group, though the test had to be continued until the plot becomes stabilized.

## Challenges

### Funnel

The study of the funnel requires applying a reasonable amount of common sense to figure out the most prominent combination of the steps. We came up with the following steps, while analysing the funnel:

1. We performed initial analysis of a basic, three-step, funnel and concluded that almost 50% users were lost before moving to the cart, while 95% of those who started building a cart proceeded with the paymnet; close to 48% of users made the entire journey from their first event to payment;
2. We cleaned the `event_log` from those devices (i.e. user observations) that could distort the result with artifacts, like, visiting a cart screen for the first time before entering the main screen for the first time accordingly;
3. Upon cleaning, we singled out from the the `event_log` a subset of those devices (users) who, when interracting with the app for the first time, saw an offer screen before they entered a cart screen (22%) and those who either never saw an offer screen or entered it after they started to arrange a cart (64%).

### Test

The test included two control groups (A/A/B), which can be considered as a more robust approach, but requires differentiating between testing the difference between the test and the control groups in isolation and combined.
